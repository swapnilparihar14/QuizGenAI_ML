{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16692\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\16692\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\16692\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import traceback\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "from keybert import KeyBERT\n",
    "# pip3 install git+https://github.com/boudinfl/pke.git\n",
    "# pip3 install flashtext\n",
    "# pip3 install - -upgrade spacy == 2.2.4\n",
    "# pip3 install keyBERT\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class MethodSelector:\n",
    "    @classmethod\n",
    "    def get_keyword_limit(cls, no_of_words, user_limit=0):\n",
    "        \"\"\"Estimates the number of keywords required\n",
    "           @input: (no_of_words)-Block of continous text on a single topic  \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (sentences)-Sentence Array\"\"\"\n",
    "        if user_limit == 0:\n",
    "            return round(no_of_words/15)\n",
    "    \n",
    "    @classmethod\n",
    "    def tokenize_sentences(cls, text):\n",
    "        \"\"\"Converts long passages into sentence array     \n",
    "           @input: (text)-Block of continous text on a single topic  \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (sentences)-Sentence Array\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
    "        return sentences\n",
    "    \n",
    "    @classmethod\n",
    "    def get_sentences_for_keyword(cls,keywords, sentences):\n",
    "        \"\"\"Maps Keywords with their sentences\n",
    "           @input: (keywords1)-Dictionary of keywords \n",
    "           @input: (sentences)-Sentence Array\n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        keyword_processor = KeywordProcessor()\n",
    "        keyword_sentences = {}\n",
    "        for word in keywords:\n",
    "            keyword_sentences[word[0]] = []\n",
    "            keyword_processor.add_keyword(word[0])\n",
    "        for sentence in sentences:\n",
    "            keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "            for key in keywords_found:\n",
    "                keyword_sentences[key].append(sentence)\n",
    "\n",
    "        for key in keyword_sentences.keys():\n",
    "            values = keyword_sentences[key]\n",
    "            values = sorted(values, key=len, reverse=True)\n",
    "            keyword_sentences[key] = values\n",
    "        return keyword_sentences\n",
    "    \n",
    "    @classmethod\n",
    "    def get_pos_for_keywords(cls,keywords1):\n",
    "        \"\"\"Generates POS for Non-POS based keywords\n",
    "           @input: (keywords1)-Dictionary of keywords \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        tagged_keywords = nltk.pos_tag(keywords1)\n",
    "        new_keyword = {}\n",
    "        for i in tagged_keywords:\n",
    "            if i[1][0] == 'N':\n",
    "                new_keyword[i[0],'n'] = keywords1[i[0]]\n",
    "            elif i[1][0] == 'V':\n",
    "                new_keyword[i[0],'v'] = keywords1[i[0]]\n",
    "            elif i[1][0] == 'J':\n",
    "                new_keyword[i[0],'a'] = keywords1[i[0]]\n",
    "\n",
    "        return new_keyword\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "class KeywordNPOS:\n",
    "    \"\"\" Sample Access Example\n",
    "        from KeyowrdAndPOSExtraction import KeywordNPOS \n",
    "        dictionary = KeywordNPOS.functionname(arguments)\"\"\"\n",
    "    @classmethod\n",
    "    def get_keywords_MultipartiteRank(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Graph based MultipartiteRank algorithm\n",
    "           @input: (text)-Block of continous text on a single topic \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        out = {}\n",
    "        try:\n",
    "            # Selecting the extractor and loading the input text in it\n",
    "            extractor = pke.unsupervised.MultipartiteRank()\n",
    "            extractor.load_document(input=text)\n",
    "\n",
    "            # It can extract these three types of \"phrases\" from the input text\n",
    "            pos = {'VERB', 'ADJ', 'NOUN'}\n",
    "            convert_pos = {'VERB': 'v', 'NOUN': 'n', 'ADJ': 'a'}\n",
    "\n",
    "            stoplist = list(string.punctuation)\n",
    "            stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "            stoplist += stopwords.words('english')\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            for partsofspeech in pos:\n",
    "                extractor.candidate_selection(\n",
    "                    pos=partsofspeech, stoplist=stoplist)\n",
    "                extractor.candidate_weighting(\n",
    "                    alpha=1.1, threshold=0.75, method='average')\n",
    "                keyphrases = extractor.get_n_best(n=keyword_limit)\n",
    "\n",
    "                for val in keyphrases:\n",
    "                    out[(val[0], convert_pos[partsofspeech])] = val[1]\n",
    "\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def get_keywords_PositionRank(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Graph based PositionRank algorithm\n",
    "            @input: (text)-Block of continous text on a single topic \n",
    "            @Hyperparameters: ()-None\n",
    "            @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "\n",
    "        out = {}\n",
    "        pos = {'NOUN', 'VERB', 'ADJ'}\n",
    "        convert_pos = {'VERB': 'v', 'NOUN': 'n', 'ADJ': 'a'}\n",
    "        try:\n",
    "            # define the grammar for selecting the keyphrase candidates\n",
    "            grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "            # 1. create a PositionRank extractor.\n",
    "            extractor = pke.unsupervised.PositionRank()\n",
    "            #print(\"Keyword extraction Using the PositionRank model\")\n",
    "\n",
    "            # 2. load the content of the document.\n",
    "            extractor.load_document(\n",
    "                input=text, language='en', normalization=None)\n",
    "\n",
    "            # 3. select the noun phrases up to 3 words as keyphrase candidates.\n",
    "            extractor.candidate_selection(\n",
    "                grammar=grammar, maximum_word_number=3)\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            # 4. weight the candidates using the sum of their word's scores that are\n",
    "            #    computed using random walk biaised with the position of the words\n",
    "            #    in the document. In the graph, nodes are words (nouns and\n",
    "            #    adjectives only) that are connected if they occur in a window of\n",
    "            #    10 words.\n",
    "            for partsofspeech in pos:\n",
    "                extractor.candidate_weighting(window=10, pos=partsofspeech)\n",
    "                # 5. get the 10-highest scored candidates as keyphrases\n",
    "                keyphrases = extractor.get_n_best(n=keyword_limit)\n",
    "                for val in keyphrases:\n",
    "                    out[(val[0], convert_pos[partsofspeech])] = val[1]\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def get_keywords_SingleRank(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Graph based SingleRank algorithm\n",
    "           @input: (text)-Block of continous text on a single topic \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        out = {}\n",
    "        try:\n",
    "            # Selecting the extractor and loading the input text in it\n",
    "            extractor = pke.unsupervised.SingleRank()\n",
    "            extractor.load_document(input=text)\n",
    "\n",
    "            # It can extract these three types of \"phrases\" from the input text\n",
    "            pos = {'NOUN', 'VERB', 'ADJ'}\n",
    "            convert_pos = {'VERB': 'v', 'NOUN': 'n', 'ADJ': 'a'}\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            for partsofspeech in pos:\n",
    "                extractor.candidate_selection(pos=partsofspeech)\n",
    "                extractor.candidate_weighting(window=10, pos=partsofspeech)\n",
    "                try:\n",
    "                    keyphrases = extractor.get_n_best(n=keyword_limit)\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "\n",
    "                for val in keyphrases:\n",
    "                    out[(val[0], convert_pos[partsofspeech])] = val[1]\n",
    "\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return out\n",
    "\n",
    "    @classmethod\n",
    "    def get_keywords_TextRank(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Graph based TextRank algorithm\n",
    "           @input: (text)-Block of continous text on a single topic \n",
    "           @Hyperparameters: ()-None\n",
    "           @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        out = {}\n",
    "        try:\n",
    "            # Selecting the extractor and loading the input text in it\n",
    "            extractor = pke.unsupervised.TextRank()\n",
    "            extractor.load_document(input=text)\n",
    "\n",
    "            # It can extract these three types of \"phrases\" from the input text\n",
    "            pos = {'NOUN', 'VERB', 'ADJ'}\n",
    "            convert_pos = {'VERB': 'v', 'NOUN': 'n', 'ADJ': 'a'}\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            for partsofspeech in pos:\n",
    "                # extractor.candidate_selection(pos=partsofspeech)\n",
    "                extractor.candidate_weighting(\n",
    "                    window=3, pos=partsofspeech, top_percent=1)\n",
    "                try:\n",
    "                    keyphrases = extractor.get_n_best(n=keyword_limit)\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "\n",
    "                for val in keyphrases:\n",
    "                    out[(val[0], convert_pos[partsofspeech])] = val[1]\n",
    "\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_keywords_YAKE(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Graph based TextRank algorithm\n",
    "            @input: (text)-Block of continous text on a single topic \n",
    "            @Hyperparameters: ()-None\n",
    "            @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        out = {}\n",
    "        try:\n",
    "            # Selecting the extractor and loading the input text in it\n",
    "            extractor = pke.unsupervised.YAKE()\n",
    "            extractor.load_document(\n",
    "                input=text, language='en', normalization=None)\n",
    "\n",
    "            # It can extract these three types of \"phrases\" from the input text\n",
    "            #pos = {'NOUN','VERB','ADJ'}\n",
    "            #convert_pos = {'VERB': 'v', 'NOUN': 'n','ADJ': 'a'}\n",
    "\n",
    "            stoplist = list(string.punctuation)\n",
    "            stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "            stoplist = stopwords.words('english')\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            extractor.candidate_selection(n=3, stoplist=stoplist)\n",
    "            extractor.candidate_weighting(\n",
    "                window=3, stoplist=stoplist, use_stems=False)\n",
    "            keyphrases = extractor.get_n_best(n=keyword_limit*2, threshold=0.8)\n",
    "\n",
    "            for val in keyphrases:\n",
    "                out[(val[0])] = val[1]\n",
    "\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return MethodSelector.get_pos_for_keywords(out)\n",
    "\n",
    "    @classmethod\n",
    "    def get_keywords_KeyBERT(cls, text,user_limit=0):\n",
    "        \"\"\"Extracts keywords from input text using Word Embedding based TextRank algorithm\n",
    "            @input: (text)-Block of continous text on a single topic \n",
    "            @Hyperparameters: ()-None\n",
    "            @Output: (out)-Dictionary with the syntax {('keyword','partofspeech'):weightage}\"\"\"\n",
    "        out = {}\n",
    "        try:\n",
    "            kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "            keyword_limit = MethodSelector.get_keyword_limit(len(text.split()),user_limit)\n",
    "\n",
    "            keywords = kw_extractor.extract_keywords(\n",
    "                text, stop_words='english', top_n=keyword_limit*2, keyphrase_ngram_range=(1, 2))\n",
    "\n",
    "            for val in keywords:\n",
    "                out[(val[0])] = val[1]\n",
    "\n",
    "        except:\n",
    "            out = {}\n",
    "            traceback.print_exc()\n",
    "\n",
    "        return MethodSelector.get_pos_for_keywords(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade spacy==2.2.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the sample input text\n",
    "text = \"\"\"There is a lot of volcanic activity at divergent plate boundaries in the oceans. For example, many undersea volcanoes are found along the Mid-Atlantic Ridge. This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean. As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust. Molten rock, called magma, erupts through these cracks onto Earth’s surface. At the surface, the molten rock is called lava. It cools and hardens, forming rock. Divergent plate boundaries also occur in the continental crust. Volcanoes form at these boundaries, but less often than in ocean crust. That’s because continental crust is thicker than oceanic crust. This makes it more difficult for molten rock to push up through the crust. Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone. The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes. When a line of volcanoes forms along a subduction zone, they make up a volcanic arc. The edges of the Pacific plate are long subduction zones lined with volcanoes. This is why the Pacific rim is called the “Pacific Ring of Fire.”\"\"\"\n",
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('atlantic ridge', 'n'): 0.04599435489729826, ('divergent plate', 'n'): 0.05051811913746707, ('plate', 'n'): 0.05675656522364285, ('divergent plate boundary', 'a'): 0.0681994202941495, ('crust', 'n'): 0.0818394401636323, ('volcanoes', 'n'): 0.08644397137081854, ('divergent', 'a'): 0.09255724673191565, ('plate boundaries', 'n'): 0.09885746497827541, ('atlantic ocean', 'v'): 0.1026581477748343, ('atlantic', 'a'): 0.10929097903133549, ('molten rock', 'n'): 0.11286214022530992, ('rock', 'n'): 0.12285379179720222, ('boundaries', 'n'): 0.12594121245326878, ('pacific', 'v'): 0.1275454360674287, ('volcanic activity', 'n'): 0.1601944782681947, ('molten', 'n'): 0.17157472445232513, ('called', 'v'): 0.17436246924346388, ('subduction', 'n'): 0.19820853169223906, ('mid', 'n'): 0.20116891760408584, ('ridge', 'n'): 0.20116891760408584, ('oceans', 'n'): 0.20903296229448917, ('pacific plate', 'v'): 0.2102683729058276, ('continental crust', 'a'): 0.2177600078479168, ('pacific ring', 'n'): 0.2181614702241551, ('lot', 'n'): 0.22856222633236592, ('activity', 'n'): 0.22856222633236592, ('volcanoes form', 'n'): 0.24117553684966264}\n",
      "\n",
      "['There is a lot of volcanic activity at divergent plate boundaries in the oceans.', 'For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.', 'This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean.', 'As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust.', 'Molten rock, called magma, erupts through these cracks onto Earth’s surface.', 'At the surface, the molten rock is called lava.', 'It cools and hardens, forming rock.', 'Divergent plate boundaries also occur in the continental crust.', 'Volcanoes form at these boundaries, but less often than in ocean crust.', 'That’s because continental crust is thicker than oceanic crust.', 'This makes it more difficult for molten rock to push up through the crust.', 'Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.', 'When a line of volcanoes forms along a subduction zone, they make up a volcanic arc.', 'The edges of the Pacific plate are long subduction zones lined with volcanoes.', 'This is why the Pacific rim is called the “Pacific Ring of Fire.”']\n",
      "\n",
      "atlantic ridge:  ['For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.']\n",
      "divergent plate:  ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.', 'Divergent plate boundaries also occur in the continental crust.']\n",
      "plate:  ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.']\n",
      "divergent plate boundary:  ['As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust.', 'This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean.']\n",
      "crust:  ['As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust.', 'This makes it more difficult for molten rock to push up through the crust.', 'Volcanoes form at these boundaries, but less often than in ocean crust.', 'That’s because continental crust is thicker than oceanic crust.']\n",
      "volcanoes:  ['The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.', 'When a line of volcanoes forms along a subduction zone, they make up a volcanic arc.', 'The edges of the Pacific plate are long subduction zones lined with volcanoes.', 'For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.']\n",
      "divergent:  []\n",
      "plate boundaries:  ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.']\n",
      "atlantic ocean:  ['This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean.']\n",
      "atlantic:  []\n",
      "molten rock:  ['Molten rock, called magma, erupts through these cracks onto Earth’s surface.', 'This makes it more difficult for molten rock to push up through the crust.', 'At the surface, the molten rock is called lava.']\n",
      "rock:  ['It cools and hardens, forming rock.']\n",
      "boundaries:  ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.', 'Volcanoes form at these boundaries, but less often than in ocean crust.', 'Divergent plate boundaries also occur in the continental crust.']\n",
      "pacific:  ['This is why the Pacific rim is called the “Pacific Ring of Fire.”']\n",
      "volcanic activity:  ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.']\n",
      "molten:  []\n",
      "called:  ['Molten rock, called magma, erupts through these cracks onto Earth’s surface.', 'This is why the Pacific rim is called the “Pacific Ring of Fire.”', 'At the surface, the molten rock is called lava.']\n",
      "subduction:  ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'When a line of volcanoes forms along a subduction zone, they make up a volcanic arc.', 'The edges of the Pacific plate are long subduction zones lined with volcanoes.']\n",
      "mid:  ['For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.']\n",
      "ridge:  []\n",
      "oceans:  ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.']\n",
      "pacific plate:  ['The edges of the Pacific plate are long subduction zones lined with volcanoes.']\n",
      "continental crust:  ['Divergent plate boundaries also occur in the continental crust.', 'That’s because continental crust is thicker than oceanic crust.']\n",
      "pacific ring:  ['This is why the Pacific rim is called the “Pacific Ring of Fire.”']\n",
      "lot:  ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.']\n",
      "activity:  []\n",
      "volcanoes form:  ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'Volcanoes form at these boundaries, but less often than in ocean crust.']\n"
     ]
    }
   ],
   "source": [
    "keywords = KeywordNPOS.get_keywords_YAKE(text)\n",
    "print(keywords)\n",
    "print()\n",
    "sentences = MethodSelector.tokenize_sentences(text)\n",
    "print(sentences)\n",
    "print()\n",
    "result  = MethodSelector.get_sentences_for_keyword(keywords,sentences)\n",
    "#print(result)\n",
    "for i in result:\n",
    "    print(f\"{i}:  {result[i]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords1 = KeywordNPOS.get_keywords_YAKE(text)\n",
    "\n",
    "\n",
    "def get_pos_forKeywords(keywords1):\n",
    "    keywords2 = KeywordNPOS.get_keywords_PositionRank(text)\n",
    "    new_keyword = {}\n",
    "    for i in keywords2:\n",
    "        if i[0] in keywords1:\n",
    "            if (i[0],'n') not in new_keyword and (i[0],'a') not in new_keyword:\n",
    "                new_keyword[(i[0],i[1])]=keywords1[i[0]]\n",
    "\n",
    "    return new_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('atlantic ridge', 'n'): 0.04599435489729826,\n",
       " ('divergent plate', 'n'): 0.05051811913746707,\n",
       " ('plate', 'n'): 0.05675656522364285,\n",
       " ('divergent plate boundary', 'a'): 0.0681994202941495,\n",
       " ('crust', 'n'): 0.0818394401636323,\n",
       " ('volcanoes', 'n'): 0.08644397137081854,\n",
       " ('divergent', 'a'): 0.09255724673191565,\n",
       " ('plate boundaries', 'n'): 0.09885746497827541,\n",
       " ('atlantic ocean', 'v'): 0.1026581477748343,\n",
       " ('atlantic', 'a'): 0.10929097903133549,\n",
       " ('molten rock', 'n'): 0.11286214022530992,\n",
       " ('rock', 'n'): 0.12285379179720222,\n",
       " ('boundaries', 'n'): 0.12594121245326878,\n",
       " ('pacific', 'v'): 0.1275454360674287,\n",
       " ('volcanic activity', 'n'): 0.1601944782681947,\n",
       " ('molten', 'n'): 0.17157472445232513,\n",
       " ('called', 'v'): 0.17436246924346388,\n",
       " ('subduction', 'n'): 0.19820853169223906,\n",
       " ('mid', 'n'): 0.20116891760408584,\n",
       " ('ridge', 'n'): 0.20116891760408584,\n",
       " ('oceans', 'n'): 0.20903296229448917,\n",
       " ('pacific plate', 'v'): 0.2102683729058276,\n",
       " ('continental crust', 'a'): 0.2177600078479168,\n",
       " ('pacific ring', 'n'): 0.2181614702241551,\n",
       " ('lot', 'n'): 0.22856222633236592,\n",
       " ('activity', 'n'): 0.22856222633236592,\n",
       " ('volcanoes form', 'n'): 0.24117553684966264}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_for_keywords(keywords1):    \n",
    "    tagged_keywords = nltk.pos_tag(keywords1)\n",
    "    new_keyword = {}\n",
    "    for i in tagged_keywords:\n",
    "        if i[1][0] == 'N':\n",
    "            new_keyword[i[0],'n'] = keywords1[i[0]]\n",
    "        elif i[1][0] == 'V':\n",
    "            new_keyword[i[0],'v'] = keywords1[i[0]]\n",
    "        elif i[1][0] == 'J':\n",
    "            new_keyword[i[0],'a'] = keywords1[i[0]]\n",
    "        \n",
    "\n",
    "    return new_keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atlantic ridge : ['For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.'] \n",
      "\n",
      "divergent plate : ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.', 'Divergent plate boundaries also occur in the continental crust.'] \n",
      "\n",
      "plate : ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.'] \n",
      "\n",
      "divergent plate boundary : ['As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust.', 'This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean.'] \n",
      "\n",
      "crust : ['As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust.', 'This makes it more difficult for molten rock to push up through the crust.', 'Volcanoes form at these boundaries, but less often than in ocean crust.', 'That’s because continental crust is thicker than oceanic crust.'] \n",
      "\n",
      "volcanoes : ['The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.', 'When a line of volcanoes forms along a subduction zone, they make up a volcanic arc.', 'The edges of the Pacific plate are long subduction zones lined with volcanoes.', 'For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.'] \n",
      "\n",
      "divergent : [] \n",
      "\n",
      "plate boundaries : ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.'] \n",
      "\n",
      "atlantic ocean : ['This is a divergent plate boundary that runs north-south through the middle of the Atlantic Ocean.'] \n",
      "\n",
      "atlantic : [] \n",
      "\n",
      "molten rock : ['Molten rock, called magma, erupts through these cracks onto Earth’s surface.', 'This makes it more difficult for molten rock to push up through the crust.', 'At the surface, the molten rock is called lava.'] \n",
      "\n",
      "rock : ['It cools and hardens, forming rock.'] \n",
      "\n",
      "boundaries : ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.', 'Volcanoes form at these boundaries, but less often than in ocean crust.', 'Divergent plate boundaries also occur in the continental crust.'] \n",
      "\n",
      "pacific : ['This is why the Pacific rim is called the “Pacific Ring of Fire.”'] \n",
      "\n",
      "volcanic activity : ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.'] \n",
      "\n",
      "molten : [] \n",
      "\n",
      "called : ['Molten rock, called magma, erupts through these cracks onto Earth’s surface.', 'This is why the Pacific rim is called the “Pacific Ring of Fire.”', 'At the surface, the molten rock is called lava.'] \n",
      "\n",
      "subduction : ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'When a line of volcanoes forms along a subduction zone, they make up a volcanic arc.', 'The edges of the Pacific plate are long subduction zones lined with volcanoes.'] \n",
      "\n",
      "mid : ['For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.'] \n",
      "\n",
      "ridge : [] \n",
      "\n",
      "oceans : ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.'] \n",
      "\n",
      "pacific plate : ['The edges of the Pacific plate are long subduction zones lined with volcanoes.'] \n",
      "\n",
      "continental crust : ['Divergent plate boundaries also occur in the continental crust.', 'That’s because continental crust is thicker than oceanic crust.'] \n",
      "\n",
      "pacific ring : ['This is why the Pacific rim is called the “Pacific Ring of Fire.”'] \n",
      "\n",
      "lot : ['There is a lot of volcanic activity at divergent plate boundaries in the oceans.'] \n",
      "\n",
      "activity : [] \n",
      "\n",
      "volcanoes form : ['Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone.', 'Volcanoes form at these boundaries, but less often than in ocean crust.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = MethodSelector.tokenize_sentences(text)\n",
    "keyword_sentences = MethodSelector.get_sentences_for_keyword(keywords1,sentences)\n",
    "for i in keyword_sentences:\n",
    "    print(f\"{i} : {keyword_sentences[i]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final class for the distractor generation for a given keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\16692\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "#!pip install sense2vec==1.0.3\n",
    "from sense2vec import Sense2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistractorSupport:\n",
    "    \"\"\"This is a helper class that provides support functions for DistractorGeneration Class\"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def wordnet_distractor_list(cls,distractor_dict,library_weight,grand_weight):\n",
    "        \"\"\"Generates the Wordnet Based distractor list\n",
    "           @input: (distractor_dict)- A dictionary of words for which we need to find distractors   \n",
    "           @Hyperparameters: (library_weight,grand_weight)-Weight of library and correction for grand child\n",
    "           @Output: (distractor_dict)-List now containing distractors as value of keywords in dictionary\"\"\"\n",
    "        \n",
    "        for element in distractor_dict:\n",
    "            #Initialization\n",
    "            distractor={}\n",
    "            word= element[0].lower()\n",
    "            orig_word = word\n",
    "            if len(word.split())>0:\n",
    "                word = word.replace(\" \",\"_\")\n",
    "                #This logic doesnt work in this library. Improvement needed\n",
    "            hypernym = element[1].hypernyms()\n",
    "\n",
    "            \n",
    "            #Distractor generation using parents \n",
    "            if len(hypernym) != 0:\n",
    "                grand_hypernym = hypernym[0].hypernyms()\n",
    "                #print(grand_hypernym)\n",
    "                for item in hypernym[0].hyponyms():\n",
    "                    name = item.lemmas()[0].name()\n",
    "                    #print (\"name \",name, \" word\",orig_word)\n",
    "                    if name == orig_word:\n",
    "                        continue\n",
    "                    name = name.replace(\"_\",\" \")\n",
    "                    name = \" \".join(w.capitalize() for w in name.split())\n",
    "                    if name is not None and name not in distractor:\n",
    "                        distractor[name] = library_weight\n",
    "            \n",
    "            #Distractor generation using grand-parents \n",
    "            if len(distractor) < 10 and len(grand_hypernym) != 0:\n",
    "                for chypernym in grand_hypernym[0].hyponyms():\n",
    "                    for item in chypernym.hyponyms():\n",
    "                        name = item.lemmas()[0].name()\n",
    "                        #print (\"name \",name, \" word\",orig_word)\n",
    "                        if name == orig_word:\n",
    "                            continue\n",
    "                        name = name.replace(\"_\",\" \")\n",
    "                        name = \" \".join(w.capitalize() for w in name.split())\n",
    "                        if name is not None and name not in distractor:\n",
    "                            distractor[name] = library_weight*grand_weight\n",
    "                            #print(distractor,\"\\n\\n\")\n",
    "\n",
    "            distractor_dict[element] = distractor\n",
    "        return distractor_dict\n",
    "    \n",
    "    @classmethod\n",
    "    def sense2vec_distractor_pruning(cls,dis):\n",
    "        \"\"\"Pruning distractor list of the sense2vec library\n",
    "           @input: (dis)- A dictionary of distractor which are repeated and too similar to original word   \n",
    "           @Hyperparameters: (library_weight,grand_weight)-None\n",
    "           @Output: (dis)-List now containing distractors as value of keywords in dictionary\"\"\"        \n",
    "        ps = PorterStemmer()\n",
    "        for i in dis:\n",
    "            splitted_word = i.split()\n",
    "            test_dict = dis[i]\n",
    "            del_list = []\n",
    "            for word in splitted_word:\n",
    "                word = ps.stem(word)\n",
    "                for distractor in test_dict:\n",
    "                    if word in distractor: \n",
    "                        del_list.append(distractor)\n",
    "                    if distractor in word:\n",
    "                        del_list.append(distractor)\n",
    "            for duplicate in del_list :\n",
    "                test_dict.pop(duplicate)\n",
    "\n",
    "            dis[i] = test_dict\n",
    "        return dis\n",
    "    \n",
    "    @classmethod\n",
    "    def get_setenence_cosine_similarity(cls,X,Y):\n",
    "        \"\"\" Generates the cosine similarity for two senteces\n",
    "            @input: (X,Y)- Two sentences\n",
    "            @Hyperparamters: ()= None\n",
    "            @Output: (cosine)- Returns the similarity measure between the two sentences\"\"\"\n",
    "        X_list = word_tokenize(X) \n",
    "        Y_list = word_tokenize(Y)\n",
    "        ps = PorterStemmer()\n",
    "        # sw contains the list of stopwords\n",
    "        stoplist = list(string.punctuation)\n",
    "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "        stoplist += stopwords.words('english')\n",
    "        l1 =[];l2 =[]\n",
    "\n",
    "        # remove stop words from the string\n",
    "        X_set = {ps.stem(w) for w in X_list if not w in stoplist} \n",
    "        Y_set = {ps.stem(w) for w in Y_list if not w in stoplist}\n",
    "\n",
    "        # form a set containing keywords of both strings \n",
    "        rvector = X_set.union(Y_set) \n",
    "        for w in rvector:\n",
    "            if w in X_set: l1.append(1) # create a vector\n",
    "            else: l1.append(0)\n",
    "            if w in Y_set: l2.append(1)\n",
    "            else: l2.append(0)\n",
    "        c = 0\n",
    "\n",
    "        # cosine formula \n",
    "        for i in range(len(rvector)):\n",
    "                c+= l1[i]*l2[i]\n",
    "        try:\n",
    "            cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "        except:\n",
    "            cosine  = 0\n",
    "\n",
    "        return cosine\n",
    "    \n",
    "\n",
    "class DistractorGeneration:\n",
    "    \"\"\"Single Responsibility of Overgenerating an exhaustive list of distractors\"\"\"\n",
    "    @classmethod\n",
    "    def get_wordnet_distractor(cls,keywords,keyword_sentences,library_weight = 1,grand_weight = 0.8):\n",
    "        \"\"\"Returns a list of distractors generated using WordNet Library\n",
    "            @input: (keyword,keyword_sentences)- Set of keywords and their corresponding sentences\n",
    "            @hyperparamter: (library_weight,grand_weight)- Weights to adjust the weightage of the library\n",
    "            @Output: (distractor_list)- List now containing distractors as value of keywords in dictionary\"\"\"\n",
    "        distractor_dict = {}\n",
    "        for element in keywords:\n",
    "            word = element[0].lower()\n",
    "            if len(word.split())>0:\n",
    "                j = 0\n",
    "                #word = word.replace(\" \",\"_\")\n",
    "                # Add different logic, this doesn't work\n",
    "            syns = wn.synsets(word,element[1])\n",
    "            if syns ==[]:\n",
    "                syns = wn.synsets(word,'n')\n",
    "            if syns ==[]:\n",
    "                syns = wn.synsets(word,'v')\n",
    "            if syns ==[]:\n",
    "                syns = wn.synsets(word,'a')\n",
    "\n",
    "\n",
    "            if len(syns) > 1:\n",
    "                cosine = 0\n",
    "                for syn in syns[::-1]:\n",
    "                    # print(element[0])\n",
    "                    # print (syn, \": \",syn.definition())\n",
    "                    # Tried with element[1] instead of the whole text. The results were not accurate\n",
    "                    intial_cosine = DistractorSupport.get_setenence_cosine_similarity(syn.definition(),text)\n",
    "                    # print(intial_cosine,\"\\n\")\n",
    "                    if cosine <= intial_cosine:\n",
    "                        cosine = intial_cosine\n",
    "                        new_syn = syn\n",
    "            elif len(syns) == 1:\n",
    "                new_syn = syns[0]\n",
    "            else:\n",
    "                new_syn =[]\n",
    "\n",
    "            str1 = \" \"\n",
    "            Y = str1.join(keyword_sentences[element[0]])\n",
    "            if new_syn and keyword_sentences[element[0]]:\n",
    "                distractor_dict[(element[0],new_syn,Y)] = 1\n",
    "            \n",
    "        return DistractorSupport.wordnet_distractor_list(distractor_dict,library_weight,grand_weight)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_distractors_conceptnet(cls,keywords,keyword_sentences,library_weight = 1):\n",
    "        \"\"\"Returns a list of distractors generated using ConceptNet Library\n",
    "            @input: (keyword,keyword_sentences)- Set of keywords and their corresponding sentences\n",
    "            @hyperparamter: (library_weight)- Weights to adjust the weightage of the library\n",
    "            @Output: (distractor_list)- List now containing distractors as value of keywords in dictionary\"\"\"\n",
    "        distractor_list = {}\n",
    "\n",
    "        for element in keywords:\n",
    "            distractor = {}\n",
    "            word = element[0].lower()\n",
    "            original_word= word\n",
    "            if (len(word.split())>0):\n",
    "                word = word.replace(\" \",\"_\")\n",
    "            url = \"http://api.conceptnet.io/query?node=/c/en/%s/%s&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,element[1],word)\n",
    "            obj = requests.get(url).json()\n",
    "\n",
    "            for edge in obj['edges']:\n",
    "                link = edge['end']['term'] \n",
    "\n",
    "                url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "                obj2 = requests.get(url2).json()\n",
    "                for edge in obj2['edges']:\n",
    "                    word2 = edge['start']['label']\n",
    "                    if word2 not in distractor and original_word.lower() not in word2.lower():\n",
    "                        distractor[word2] = library_weight\n",
    "            distractor_list[word] = distractor\n",
    "\n",
    "        return distractor_list\n",
    "    \n",
    "    def sense2vec_get_words(cls,keywords,keyword_sentences,library_weight = 1):\n",
    "        \"\"\"Returns a list of distractors generated using Sense2vec Library\n",
    "            @input: (keyword,keyword_sentences)- Set of keywords and their corresponding sentences\n",
    "            @hyperparamter: (library_weight)- Weights to adjust the weightage of the library\n",
    "            @Output: (distractor_list)- List now containing distractors as value of keywords in dictionary\"\"\"\n",
    "        \n",
    "        s2v = Sense2Vec().from_disk('s2v_old')\n",
    "        distractor_list = {}\n",
    "        for element in keywords:\n",
    "            output = {}\n",
    "            #print(element[0])\n",
    "            word = element[0].lower()\n",
    "            word = word.replace(\" \", \"_\")\n",
    "            most_similar = []\n",
    "            sense = s2v.get_best_sense(word)\n",
    "            distractor_limit=10\n",
    "            while(distractor_limit>2):\n",
    "                try:\n",
    "                    most_similar = s2v.most_similar(sense, n=distractor_limit)\n",
    "                    break\n",
    "                except:\n",
    "                    distractor_limit -= 2\n",
    "\n",
    "            # print (\"most_similar \",most_similar)\n",
    "\n",
    "            for each_word in most_similar:\n",
    "                append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "                if append_word.lower() != word:\n",
    "                    output[append_word] = each_word[1] * library_weight\n",
    "\n",
    "            distractor_list[element[0]] = output\n",
    "            #print(f\"{element[0]}:{distractor_list[element[0]]} \")\n",
    "        return DistractorSupport.sense2vec_distractor_pruning(distractor_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plate', Synset('plate.n.06'), 'Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone. The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes.'):{'Horst': 1, 'Corn Snow': 0.8, 'Crud': 0.8, 'Bed': 0.8, 'Cambium': 0.8, 'Dermis': 0.8, 'Epidermis': 0.8, 'Horizon': 0.8, 'Seam': 0.8, 'Stratum Corneum': 0.8, 'Stratum Germinativum': 0.8, 'Stratum Granulosum': 0.8, 'Stratum Lucidum': 0.8, 'Substrate': 0.8, 'Superstrate': 0.8, 'Wall': 0.8, 'Floor': 0.8} \n",
      "\n",
      "\n",
      "('crust', Synset('crust.n.01'), 'As tectonic plates pull away from each other at a divergent plate boundary, they create deep fissures, or cracks, in the crust. This makes it more difficult for molten rock to push up through the crust. Volcanoes form at these boundaries, but less often than in ocean crust. That’s because continental crust is thicker than oceanic crust.'):{'Asthenosphere': 1, 'Cell Wall': 1, 'Chromosphere': 1, 'Exosphere': 1, 'Hydrosphere': 1, 'Lithosphere': 1, 'Lower Mantle': 1, 'Mantle': 1, 'Mesosphere': 1, 'Ozone Layer': 1, 'Snow': 1, 'Stratosphere': 1, 'Stratum': 1, 'Surface': 1, 'Thermosphere': 1, 'Tropopause': 1, 'Troposphere': 1, 'Upper Mantle': 1} \n",
      "\n",
      "\n",
      "('volcanoes', Synset('vent.n.03'), 'The leading edge of the plate melts as it is pulled into the mantle, forming magma that erupts as volcanoes. When a line of volcanoes forms along a subduction zone, they make up a volcanic arc. The edges of the Pacific plate are long subduction zones lined with volcanoes. For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.'):{'Chap': 1, 'Chink': 1, 'Crevasse': 1, 'Fatigue Crack': 1, 'Fault': 1, 'Rift': 1, 'Slit': 1, 'Split': 1, 'Vent': 1, 'Abyss': 0.8, 'Gulf': 0.8, 'Foramen Magnum': 0.8, 'Interventricular Foramen': 0.8, 'Aperture': 0.8, 'Bolt-hole': 0.8, 'Bullet Hole': 0.8, 'Cranny': 0.8, 'Knothole': 0.8, 'Leak': 0.8, 'Ozone Hole': 0.8, 'Perforation': 0.8, 'Rathole': 0.8, 'Corner Pocket': 0.8, 'Side Pocket': 0.8} \n",
      "\n",
      "\n",
      "('rock', Synset('rock.n.02'), 'It cools and hardens, forming rock.'):{'Abrasive': 1, 'Adhesive Material': 1, 'Aggregate': 1, 'Ammunition': 1, 'Animal Material': 1, 'Atom': 1, 'Ballast': 1, 'Bedding Material': 1, 'Bimetal': 1, 'Builder': 1, 'Chemical': 1, 'Coloring Material': 1, 'Composite Material': 1, 'Conductor': 1, 'Contaminant': 1, 'Detritus': 1, 'Diamagnet': 1, 'Discharge': 1, 'Dust': 1, 'Earth': 1, 'Elastomer': 1, 'Fiber': 1, 'Filling': 1, 'Floccule': 1, 'Fluff': 1, 'Foam': 1, 'Hazmat': 1, 'Homogenate': 1, 'Humate': 1, 'Impregnation': 1, 'Insulator': 1, 'Mineral': 1, 'Packing Material': 1, 'Paper': 1, 'Particulate': 1, 'Plant Material': 1, 'Precursor': 1, 'Radioactive Material': 1, 'Raw Material': 1, 'Rind': 1, 'Sealing Material': 1, 'Sorbate': 1, 'Sorbent': 1, 'Thickening': 1, 'Toner': 1, 'Transparent Substance': 1, 'Undercut': 1, 'Vernix': 1, 'Wad': 1, 'Waste': 1} \n",
      "\n",
      "\n",
      "('boundaries', Synset('boundary.n.02'), 'There is a lot of volcanic activity at divergent plate boundaries in the oceans. Volcanoes form at these boundaries, but less often than in ocean crust. Divergent plate boundaries also occur in the continental crust.'):{'Boundary': 1, 'Centerline': 1, 'Curve': 1, 'Geodesic': 1, 'Perimeter': 1, 'Straight Line': 1, 'Blob': 0.8, 'Shapelessness': 0.8, 'Space': 0.8, 'Fold': 0.8, 'Furcation': 0.8, 'Jog': 0.8, 'Zigzag': 0.8, 'Columella': 0.8, 'Hoodoo': 0.8, 'Articulation': 0.8, 'Node': 0.8, 'Knot': 0.8, 'Tortuosity': 0.8, 'Warp': 0.8, 'Equilateral': 0.8, 'Parallel': 0.8, 'Pencil': 0.8, 'Plane Figure': 0.8, 'Solid Figure': 0.8, 'Subfigure': 0.8, 'Leaf Shape': 0.8, 'Cartesian Plane': 0.8, 'Facet Plane': 0.8, 'Midplane': 0.8, 'Orbital Plane': 0.8, 'Picture Plane': 0.8, 'Tangent Plane': 0.8, 'Bulb': 0.8, 'Coil': 0.8, 'Cone': 0.8, 'Cylinder': 0.8, 'Disk': 0.8, 'Rim': 0.8, 'Sphere': 0.8, 'Spheroid': 0.8, 'Torus': 0.8, 'Whirl': 0.8, 'Block': 0.8, 'Cast': 0.8, 'Concave Shape': 0.8, 'Convex Shape': 0.8, 'Frustum': 0.8, 'Ovoid': 0.8, 'Polyhedron': 0.8, 'Toroid': 0.8} \n",
      "\n",
      "\n",
      "('pacific', Synset('pacific.n.01'), 'This is why the Pacific rim is called the “Pacific Ring of Fire.”'):{'Blob': 0.8, 'Shapelessness': 0.8, 'Space': 0.8, 'Fold': 0.8, 'Furcation': 0.8, 'Jog': 0.8, 'Zigzag': 0.8, 'Columella': 0.8, 'Hoodoo': 0.8, 'Articulation': 0.8, 'Node': 0.8, 'Knot': 0.8, 'Tortuosity': 0.8, 'Warp': 0.8, 'Equilateral': 0.8, 'Parallel': 0.8, 'Pencil': 0.8, 'Plane Figure': 0.8, 'Solid Figure': 0.8, 'Subfigure': 0.8, 'Boundary': 0.8, 'Centerline': 0.8, 'Curve': 0.8, 'Geodesic': 0.8, 'Perimeter': 0.8, 'Straight Line': 0.8, 'Leaf Shape': 0.8, 'Cartesian Plane': 0.8, 'Facet Plane': 0.8, 'Midplane': 0.8, 'Orbital Plane': 0.8, 'Picture Plane': 0.8, 'Tangent Plane': 0.8, 'Bulb': 0.8, 'Coil': 0.8, 'Cone': 0.8, 'Cylinder': 0.8, 'Disk': 0.8, 'Rim': 0.8, 'Sphere': 0.8, 'Spheroid': 0.8, 'Torus': 0.8, 'Whirl': 0.8, 'Block': 0.8, 'Cast': 0.8, 'Concave Shape': 0.8, 'Convex Shape': 0.8, 'Frustum': 0.8, 'Ovoid': 0.8, 'Polyhedron': 0.8, 'Toroid': 0.8} \n",
      "\n",
      "\n",
      "('called', Synset('call.v.13'), 'Molten rock, called magma, erupts through these cracks onto Earth’s surface. This is why the Pacific rim is called the “Pacific Ring of Fire.” At the surface, the molten rock is called lava.'):{'Call': 1, 'Lay Over': 1, 'Interrupt': 0.8, 'Barrage Jam': 0.8, 'Blanket Jam': 0.8, 'Point Jam': 0.8, 'Spot Jam': 0.8, 'Rest': 0.8, 'Take Five': 0.8, 'Take Ten': 0.8} \n",
      "\n",
      "\n",
      "('subduction', Synset('subduction.n.01'), 'Many volcanoes form along convergent plate boundaries where one tectonic plate is pulled down beneath another at a subduction zone. When a line of volcanoes forms along a subduction zone, they make up a volcanic arc. The edges of the Pacific plate are long subduction zones lined with volcanoes.'):{'Alluvion': 1, 'Desertification': 1, 'Diastrophism': 1, 'Erosion': 1, 'Fold': 1, 'Glaciation': 1, 'Intrusion': 1, 'Metamorphism': 1, 'Orogeny': 1, 'Stratification': 1} \n",
      "\n",
      "\n",
      "('mid', Synset('mid.s.01'), 'For example, many undersea volcanoes are found along the Mid-Atlantic Ridge.'):{'Ultracentrifugation': 0.8, 'Acylation': 0.8, 'Agglutination': 0.8, 'Amylolysis': 0.8, 'Association': 0.8, 'Bluing': 0.8, 'Calcification': 0.8, 'Catalysis': 0.8, 'Chelation': 0.8, 'Chemical Reaction': 0.8, 'Chlorination': 0.8, 'Cleavage': 0.8, 'Corrosion': 0.8, 'Cracking': 0.8, 'De-iodination': 0.8, 'Deamination': 0.8, 'Decalcification': 0.8, 'Decarboxylation': 0.8, 'Demineralization': 0.8, 'Desalination': 0.8, 'Digestion': 0.8, 'Dissociation': 0.8, 'Gasification': 0.8, 'Gassing': 0.8, 'Hydrogenation': 0.8, 'Intumescence': 0.8, 'Inversion': 0.8, 'Iodination': 0.8, 'Mechanism': 0.8, 'Nitrification': 0.8, 'Peptization': 0.8, 'Photosynthesis': 0.8, 'Polymerization': 0.8, 'Precipitation': 0.8, 'Proteolysis': 0.8, 'Pyrochemical Process': 0.8, 'Sequestration': 0.8, 'Syneresis': 0.8, 'Synthesis': 0.8, 'Transamination': 0.8, 'Zymosis': 0.8, 'Column Chromatography': 0.8, 'Paper Chromatography': 0.8, 'Blood Coagulation': 0.8, 'Thermocoagulation': 0.8, 'Cavity': 0.8, 'Corruption': 0.8, 'Decomposition': 0.8, 'Dilapidation': 0.8, 'Spoilage': 0.8, 'Degaussing': 0.8, 'Osmosis': 0.8, 'Permeation': 0.8, 'Transport': 0.8, 'Fibrinolysis': 0.8, 'Lysis': 0.8, 'Leeway': 0.8, 'Immunoelectrophoresis': 0.8, 'Paper Electrophoresis': 0.8, 'Decoction': 0.8, 'Dehydration': 0.8, 'Elution': 0.8, 'Infusion': 0.8, 'Mineral Extraction': 0.8, 'Negative Feedback': 0.8, 'Positive Feedback': 0.8, 'Percolation': 0.8, 'Filling': 0.8, 'Flowage': 0.8, 'Inflow': 0.8, 'Outflow': 0.8, 'Slipstream': 0.8, 'Streamline Flow': 0.8, 'Turbulent Flow': 0.8, 'Incrustation': 0.8, 'Reticulation': 0.8, 'Petrifaction': 0.8, 'Alluvion': 0.8, 'Desertification': 0.8, 'Diastrophism': 0.8, 'Erosion': 0.8, 'Fold': 0.8, 'Glaciation': 0.8, 'Intrusion': 0.8, 'Metamorphism': 0.8, 'Orogeny': 0.8, 'Stratification': 0.8, 'Subduction': 0.8, 'Congealment': 0.8, 'Chain Reaction': 0.8, 'Decay': 0.8, 'Endoergic Reaction': 0.8, 'Exoergic Reaction': 0.8, 'Fission': 0.8, 'Fusion': 0.8, 'Spallation': 0.8, 'Libration': 0.8, 'Freeze': 0.8, 'Liquefaction': 0.8, 'Thaw': 0.8, 'Vaporization': 0.8, 'Adaptive Radiation': 0.8, 'Emission': 0.8, 'Heat Sink': 0.8, 'Maceration': 0.8, 'Absorption': 0.8, 'Adsorption': 0.8, 'Origin': 0.8, 'Rigor Mortis': 0.8, 'Activation': 0.8, 'Galvanization': 0.8, 'Potentiation': 0.8, 'Advection': 0.8, 'Climate Change': 0.8, 'Convection': 0.8, 'Cooling': 0.8, 'Heating': 0.8, 'Microphoning': 0.8} \n",
      "\n",
      "\n",
      "('oceans', Synset('ocean.n.01'), 'There is a lot of volcanic activity at divergent plate boundaries in the oceans.'):{'Backwater': 1, 'Bay': 1, 'Channel': 1, 'Drink': 1, 'Estuary': 1, 'Flowage': 1, 'Ford': 1, 'Gulf': 1, 'High Sea': 1, 'Inlet': 1, 'Lake': 1, 'Main': 1, 'Mid-water': 1, 'Ocean': 1, 'Offing': 1, 'Polynya': 1, 'Pool': 1, 'Sea': 1, 'Seven Seas': 1, 'Shoal': 1, 'Sound': 1, 'Stream': 1, 'Territorial Waters': 1, 'Waterfall': 1, 'Waterway': 1} \n",
      "\n",
      "\n",
      "('lot', Synset('lot.n.02'), 'There is a lot of volcanic activity at divergent plate boundaries in the oceans.'):{'Baseball Diamond': 1, 'Battlefield': 1, 'Breeding Ground': 1, 'Center Field': 1, 'Clearing': 1, 'Desert': 1, 'Fairground': 1, 'Fairway': 1, 'Field': 1, 'Field Of Fire': 1, 'Grassland': 1, 'Grounds': 1, 'Industrial Park': 1, 'Left Field': 1, 'Midway': 1, 'Mine Field': 1, 'Minefield': 1, 'Mud Flat': 1, 'Oasis': 1, 'Outfield': 1, 'Parade Ground': 1, 'Park': 1, 'Picnic Area': 1, 'Playing Field': 1, 'Plot': 1, 'Public Square': 1, 'Range': 1, 'Right Field': 1, 'Sector': 1, 'Short': 1, 'Site': 1, 'Subdivision': 1, 'Terrain': 1, 'Toll Plaza': 1, 'Yard': 1} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k = DistractorGeneration.get_wordnet_distractor(keywords,keyword_sentences)\n",
    "\n",
    "for i in k:\n",
    "    print(f\"{i}:{k[i]} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_distractors_conceptnet(keywords,keyword_sentences,library_weight = 1,grand_weight = 0.8):\n",
    "    distractor_list = {}\n",
    "    \n",
    "    for element in keywords:\n",
    "        distractor = {}\n",
    "        word = element[0].lower()\n",
    "        original_word= word\n",
    "        if (len(word.split())>0):\n",
    "            word = word.replace(\" \",\"_\")\n",
    "        url = \"http://api.conceptnet.io/query?node=/c/en/%s/%s&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,element[1],word)\n",
    "        obj = requests.get(url).json()\n",
    "\n",
    "        for edge in obj['edges']:\n",
    "            link = edge['end']['term'] \n",
    "\n",
    "            url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "            obj2 = requests.get(url2).json()\n",
    "            for edge in obj2['edges']:\n",
    "                word2 = edge['start']['label']\n",
    "                if word2 not in distractor and original_word.lower() not in word2.lower():\n",
    "                    distractor[word2] = library_weight\n",
    "        distractor_list[word] = distractor\n",
    "                   \n",
    "    return distractor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('field_game.n.01')]\n",
      "[Synset('outdoor_game.n.01')]\n",
      "{'Ball Game': 1, 'Field Hockey': 1, 'Football': 1, 'Hurling': 1, 'Lacrosse': 1, 'Polo': 1, 'Pushball': 1, 'Ultimate Frisbee': 1, 'Clock Golf': 0.8, 'Match Play': 0.8, 'Medal Play': 0.8, 'Miniature Golf': 0.8, 'Professional Golf': 0.8, 'Round Of Golf': 0.8}\n"
     ]
    }
   ],
   "source": [
    "library_weight = 1\n",
    "grand_weight = 0.8\n",
    "distractor={}\n",
    "\n",
    "temp_distractors = []\n",
    "word= \"cricket\"\n",
    "syns = wn.synsets(word,'n')\n",
    "syn = syns[1]\n",
    "orig_word = word\n",
    "if len(word.split())>0:\n",
    "    word = word.replace(\" \",\"_\")\n",
    "hypernym = syn.hypernyms()\n",
    "print(hypernym)\n",
    "if len(hypernym) != 0:\n",
    "    grand_hypernym = hypernym[0].hypernyms()\n",
    "    print(grand_hypernym)\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractor:\n",
    "            distractor[name] = library_weight\n",
    "if len(distractor) < 10 and len(grand_hypernym) != 0:\n",
    "    for chypernym in grand_hypernym[0].hyponyms():\n",
    "        for item in chypernym.hyponyms():\n",
    "            name = item.lemmas()[0].name()\n",
    "            #print (\"name \",name, \" word\",orig_word)\n",
    "            if name == orig_word:\n",
    "                continue\n",
    "            name = name.replace(\"_\",\" \")\n",
    "            name = \" \".join(w.capitalize() for w in name.split())\n",
    "            if name is not None and name not in distractor:\n",
    "                distractor[name] = library_weight*grand_weight\n",
    "        \n",
    "print(distractor)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "absl-py                            0.11.0\n",
      "admin                              0.0.1\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 1.9.12\n",
      "anaconda-project                   0.8.3\n",
      "appdirs                            1.4.4\n",
      "argcomplete                        1.10.0\n",
      "argh                               0.26.2\n",
      "asn1crypto                         1.3.0\n",
      "astroid                            2.3.3\n",
      "astropy                            4.0.1.post1\n",
      "astunparse                         1.6.3\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              20.3.0\n",
      "audioread                          2.1.9\n",
      "autopep8                           1.5.1\n",
      "Babel                              2.8.0\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.1\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "bcrypt                             3.1.7\n",
      "beautifulsoup4                     4.8.0\n",
      "bitarray                           1.4.0\n",
      "bkcharts                           0.2\n",
      "bleach                             3.1.5\n",
      "blis                               0.4.1\n",
      "bokeh                              2.1.1\n",
      "boto                               2.49.0\n",
      "boto3                              1.17.56\n",
      "botocore                           1.20.56\n",
      "Bottleneck                         1.3.2\n",
      "brotlipy                           0.7.0\n",
      "cachetools                         4.1.1\n",
      "catalogue                          1.0.0\n",
      "certifi                            2020.6.20\n",
      "cffi                               1.14.0\n",
      "chardet                            3.0.4\n",
      "click                              7.1.2\n",
      "cloudpickle                        1.5.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.3\n",
      "comtypes                           1.1.7\n",
      "conda                              4.9.1\n",
      "conda-build                        3.18.11\n",
      "conda-package-handling             1.7.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cryptography                       2.9.2\n",
      "cycler                             0.10.0\n",
      "cymem                              2.0.5\n",
      "Cython                             0.29.21\n",
      "cytoolz                            0.10.1\n",
      "dask                               2.20.0\n",
      "dataclasses                        0.6\n",
      "decorator                          4.4.2\n",
      "defusedxml                         0.6.0\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2.20.0\n",
      "docutils                           0.16\n",
      "docx2txt                           0.8\n",
      "EbookLib                           0.17.1\n",
      "emoji                              0.6.0\n",
      "en                                 0.0.1\n",
      "en-core-web-sm                     2.2.5\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "extract-msg                        0.23.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.8.3\n",
      "flashtext                          2.7\n",
      "Flask                              1.1.2\n",
      "fsspec                             0.7.4\n",
      "funcy                              1.15\n",
      "future                             0.18.2\n",
      "gast                               0.3.3\n",
      "gevent                             20.6.2\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "google-auth                        1.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-auth-oauthlib               0.4.2\n",
      "google-pasta                       0.2.0\n",
      "greenlet                           0.4.16\n",
      "grpcio                             1.33.2\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html2text                          2020.1.16\n",
      "html5lib                           1.1\n",
      "idna                               2.10\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "IMAPClient                         2.1.0\n",
      "importlib-metadata                 1.7.0\n",
      "iniconfig                          1.1.1\n",
      "intervaltree                       3.0.2\n",
      "ipykernel                          5.3.2\n",
      "ipympl                             0.6.3\n",
      "ipython                            7.16.1\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.6.3\n",
      "isort                              4.3.21\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.17.1\n",
      "Jinja2                             2.11.2\n",
      "jmespath                           0.10.0\n",
      "joblib                             0.16.0\n",
      "json5                              0.9.5\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.6\n",
      "jupyter-console                    6.1.0\n",
      "jupyter-core                       4.6.3\n",
      "jupyterlab                         2.1.5\n",
      "jupyterlab-server                  1.2.0\n",
      "jupyterlab-widgets                 1.0.0\n",
      "Keras                              2.4.3\n",
      "Keras-Preprocessing                1.1.2\n",
      "keybert                            0.2.0\n",
      "keyring                            21.2.1\n",
      "kiwisolver                         1.2.0\n",
      "lazy-object-proxy                  1.4.3\n",
      "libarchive-c                       2.9\n",
      "librosa                            0.8.0\n",
      "llvmlite                           0.33.0+1.g022ab0f\n",
      "locket                             0.2.0\n",
      "lxml                               4.5.2\n",
      "Markdown                           3.3.3\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.2.2\n",
      "mccabe                             0.6.1\n",
      "menuinst                           1.4.16\n",
      "missingno                          0.4.2\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.1.0\n",
      "mkl-random                         1.1.1\n",
      "mkl-service                        2.3.0\n",
      "mlxtend                            0.17.3\n",
      "mock                               4.0.2\n",
      "more-itertools                     8.4.0\n",
      "mpmath                             1.1.0\n",
      "msgpack                            1.0.0\n",
      "multipledispatch                   0.6.0\n",
      "murmurhash                         1.0.5\n",
      "navigator-updater                  0.2.1\n",
      "nbconvert                          5.6.1\n",
      "nbformat                           5.0.7\n",
      "networkx                           2.4\n",
      "neuralcoref                        4.0\n",
      "nltk                               3.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.0.3\n",
      "numba                              0.50.1\n",
      "numexpr                            2.7.1\n",
      "numpy                              1.18.5\n",
      "numpydoc                           1.1.0\n",
      "oauthlib                           3.1.0\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.4\n",
      "opt-einsum                         3.3.0\n",
      "packaging                          20.8\n",
      "pandas                             1.0.5\n",
      "pandocfilters                      1.4.2\n",
      "paramiko                           2.7.1\n",
      "parso                              0.7.0\n",
      "partd                              1.1.0\n",
      "path                               13.1.0\n",
      "pathlib2                           2.3.5\n",
      "pathtools                          0.1.2\n",
      "pathy                              0.4.0\n",
      "patsy                              0.5.1\n",
      "pdfminer.six                       20181108\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             7.2.0\n",
      "pip                                20.1.1\n",
      "pke                                1.8.1\n",
      "pkginfo                            1.5.0.1\n",
      "plac                               0.9.6\n",
      "plotly                             4.14.3\n",
      "plotly-express                     0.4.1\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "pooch                              1.3.0\n",
      "preshed                            3.0.5\n",
      "prometheus-client                  0.8.0\n",
      "prompt-toolkit                     3.0.5\n",
      "protobuf                           3.13.0\n",
      "psutil                             5.7.0\n",
      "py                                 1.10.0\n",
      "pyasn1                             0.4.8\n",
      "pyasn1-modules                     0.2.8\n",
      "pycodestyle                        2.5.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycryptodome                       3.10.1\n",
      "pycurl                             7.43.0.5\n",
      "pydantic                           1.7.3\n",
      "pydocstyle                         5.0.2\n",
      "pyflakes                           2.2.0\n",
      "Pygments                           2.6.1\n",
      "pyLDAvis                           2.1.2\n",
      "pylint                             2.4.4\n",
      "PyNaCl                             1.4.0\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          19.1.0\n",
      "pyparsing                          2.4.7\n",
      "PyPDF2                             1.26.0\n",
      "pyreadline                         2.1\n",
      "pyrsistent                         0.16.0\n",
      "PySocks                            1.7.1\n",
      "pytest                             6.2.1\n",
      "pytest-runner                      5.2\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.3.4\n",
      "python-language-server             0.34.1\n",
      "python-louvain                     0.14\n",
      "python-pptx                        0.6.18\n",
      "python-utils                       2.4.0\n",
      "pytz                               2020.1\n",
      "PyWavelets                         1.1.1\n",
      "pywin32                            227\n",
      "pywin32-ctypes                     0.2.0\n",
      "pywinpty                           0.5.7\n",
      "PyYAML                             5.3.1\n",
      "pyzmq                              19.0.1\n",
      "QDarkStyle                         2.8.1\n",
      "QtAwesome                          0.7.2\n",
      "qtconsole                          4.7.5\n",
      "QtPy                               1.9.0\n",
      "regex                              2020.6.8\n",
      "requests                           2.24.0\n",
      "requests-oauthlib                  1.3.0\n",
      "resampy                            0.2.2\n",
      "retrying                           1.3.3\n",
      "rope                               0.17.0\n",
      "rsa                                4.6\n",
      "Rtree                              0.9.4\n",
      "ruamel-yaml                        0.15.87\n",
      "s3transfer                         0.4.2\n",
      "sacremoses                         0.0.45\n",
      "scikit-image                       0.16.2\n",
      "scikit-learn                       0.23.1\n",
      "scikit-plot                        0.3.7\n",
      "scikit-surprise                    1.1.1\n",
      "scipy                              1.5.0\n",
      "seaborn                            0.10.1\n",
      "Send2Trash                         1.5.0\n",
      "sense2vec                          1.0.3\n",
      "sentence-transformers              1.0.4\n",
      "sentencepiece                      0.1.95\n",
      "setuptools                         49.2.0.post20200714\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.4.0.3\n",
      "sip                                4.19.13\n",
      "six                                1.15.0\n",
      "sklearn                            0.0\n",
      "smart-open                         3.0.0\n",
      "snowballstemmer                    2.0.0\n",
      "sortedcollections                  1.2.1\n",
      "sortedcontainers                   2.2.2\n",
      "SoundFile                          0.10.3.post1\n",
      "soupsieve                          2.0.1\n",
      "spacy                              2.2.3\n",
      "spacy-legacy                       3.0.1\n",
      "SpeechRecognition                  3.8.1\n",
      "Sphinx                             3.1.2\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             1.0.3\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.4\n",
      "sphinxcontrib-websupport           1.2.3\n",
      "spyder                             4.1.4\n",
      "spyder-kernels                     1.9.2\n",
      "SQLAlchemy                         1.3.18\n",
      "srsly                              1.0.5\n",
      "statsmodels                        0.11.1\n",
      "surprise                           0.1\n",
      "sympy                              1.6.1\n",
      "tables                             3.6.1\n",
      "tblib                              1.6.0\n",
      "tensorboard                        2.3.0\n",
      "tensorboard-plugin-wit             1.7.0\n",
      "tensorflow                         2.3.1\n",
      "tensorflow-estimator               2.3.0\n",
      "termcolor                          1.1.0\n",
      "terminado                          0.8.3\n",
      "testpath                           0.4.4\n",
      "text2emotion                       0.0.5\n",
      "textract                           1.6.3\n",
      "thinc                              7.3.1\n",
      "threadpoolctl                      2.1.0\n",
      "tokenizers                         0.10.2\n",
      "toml                               0.10.2\n",
      "toolz                              0.10.0\n",
      "torch                              1.7.0\n",
      "torchvision                        0.8.1\n",
      "tornado                            6.0.4\n",
      "tqdm                               4.47.0\n",
      "traitlets                          4.3.3\n",
      "transformers                       4.5.1\n",
      "typer                              0.3.2\n",
      "typing-extensions                  3.7.4.2\n",
      "tzlocal                            1.5.1\n",
      "ujson                              1.35\n",
      "unicodecsv                         0.14.1\n",
      "Unidecode                          1.2.0\n",
      "urllib3                            1.25.9\n",
      "wasabi                             0.8.2\n",
      "watchdog                           0.10.3\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           1.0.1\n",
      "wheel                              0.34.2\n",
      "widgetsnbextension                 3.5.1\n",
      "win-inet-pton                      1.1.0\n",
      "win-unicode-console                0.5\n",
      "wincertstore                       0.2\n",
      "wrapt                              1.11.2\n",
      "xgboost                            1.2.0\n",
      "xlrd                               1.2.0\n",
      "XlsxWriter                         1.2.9\n",
      "xlwings                            0.19.5\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yapf                               0.30.0\n",
      "yellowbrick                        1.1\n",
      "zict                               2.0.0\n",
      "zipp                               3.1.0\n",
      "zope.event                         4.4\n",
      "zope.interface                     4.7.1\n"
     ]
    }
   ],
   "source": [
    "def sense2vec_get_words(keywords,keyword_sentences,library_weight = 1,grand_weight = 0.8):\n",
    "    s2v = Sense2Vec().from_disk('s2v_old')\n",
    "    distractor_list = {}\n",
    "    for element in keywords:\n",
    "        output = {}\n",
    "        #print(element[0])\n",
    "        word = element[0].lower()\n",
    "        word = word.replace(\" \", \"_\")\n",
    "        most_similar = []\n",
    "        sense = s2v.get_best_sense(word)\n",
    "        distractor_limit=10\n",
    "        while(distractor_limit>2):\n",
    "            try:\n",
    "                most_similar = s2v.most_similar(sense, n=distractor_limit)\n",
    "                break\n",
    "            except:\n",
    "                distractor_limit -= 2\n",
    "\n",
    "        # print (\"most_similar \",most_similar)\n",
    "\n",
    "        for each_word in most_similar:\n",
    "            append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \").lower()\n",
    "            if append_word.lower() != word:\n",
    "                output[append_word] = each_word[1] * library_weight\n",
    "\n",
    "        distractor_list[element[0]] = output\n",
    "        #print(f\"{element[0]}:{distractor_list[element[0]]} \")\n",
    "    return distractor_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('atlantic ridge', 'n'): 0.04599435489729826, ('divergent plate', 'n'): 0.05051811913746707, ('plate', 'n'): 0.05675656522364285, ('divergent plate boundary', 'a'): 0.0681994202941495, ('crust', 'n'): 0.0818394401636323, ('volcanoes', 'n'): 0.08644397137081854, ('divergent', 'a'): 0.09255724673191565, ('plate boundaries', 'n'): 0.09885746497827541, ('atlantic ocean', 'v'): 0.1026581477748343, ('atlantic', 'a'): 0.10929097903133549, ('molten rock', 'n'): 0.11286214022530992, ('rock', 'n'): 0.12285379179720222, ('boundaries', 'n'): 0.12594121245326878, ('pacific', 'v'): 0.1275454360674287, ('volcanic activity', 'n'): 0.1601944782681947, ('molten', 'n'): 0.17157472445232513, ('called', 'v'): 0.17436246924346388, ('subduction', 'n'): 0.19820853169223906, ('mid', 'n'): 0.20116891760408584, ('ridge', 'n'): 0.20116891760408584, ('oceans', 'n'): 0.20903296229448917, ('pacific plate', 'v'): 0.2102683729058276, ('continental crust', 'a'): 0.2177600078479168, ('pacific ring', 'n'): 0.2181614702241551, ('lot', 'n'): 0.22856222633236592, ('activity', 'n'): 0.22856222633236592, ('volcanoes form', 'n'): 0.24117553684966264}\n"
     ]
    }
   ],
   "source": [
    "print(keywords)\n",
    "d = sense2vec_get_words(keywords,keyword_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "#print(element[0])\n",
    "word = 'crust'\n",
    "word = word.replace(\" \", \"_\")\n",
    "s2v = Sense2Vec().from_disk('s2v_old')\n",
    "sense = s2v.get_best_sense(word)\n",
    "distractor_limit=10\n",
    "most_similar = []\n",
    "while(distractor_limit>2):\n",
    "    try:\n",
    "        most_similar = s2v.most_similar(sense, n=distractor_limit)\n",
    "        break\n",
    "    except:\n",
    "        distractor_limit -= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralcoref.add_to_pipe(nlp)\n",
    "doc = nlp(text)\n",
    "resolved_text = doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install neuralcoref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
